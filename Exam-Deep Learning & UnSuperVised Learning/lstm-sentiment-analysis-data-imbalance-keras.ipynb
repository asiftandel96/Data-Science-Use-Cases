{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "50bbbd47a98fd909d5838623ad52ef7cf28702a3"
   },
   "source": [
    " \n",
    "**Sentiment Analysis:**<br> \n",
    "The process of computationally identifying and categorizing opinions expressed in a piece of text, especially in order to determine whether the writer's attitude towards a particular topic, product, etc. is positive, negative, or neutral. In common ML words its just a classification problem. \n",
    "\n",
    "**What is class imbalance:**<br>\n",
    "It is the problem in machine learning where the total number of a class of data (positive) is far less than the total number of another class of data (negative). This problem is extremely common in practice and can be observed in various disciplines including fraud detection, anomaly detection, medical diagnosis, oil spillage detection, facial recognition, etc.\n",
    "\n",
    "\n",
    "**Solving class imbalanced data:**<br>\n",
    "I am using the two most effective ways to mitigate this:<br>\n",
    "- Up sampling \n",
    "- Using class weighted loss function\n",
    "\n",
    "**Dataset**<br>\n",
    "First GOP Debate Twitter Sentiment\n",
    "About this Dataset\n",
    "This data originally came from [Crowdflower's Data for Everyone library ](http://www.crowdflower.com/data-for-everyone).\n",
    "\n",
    "> As the original source says,\n",
    "> We looked through tens of thousands of tweets about the early August GOP debate in Ohio and asked contributors to do both\n",
    "> sentiment analysis and data categorization. Contributors were asked if the tweet was relevant, which candidate was mentioned,\n",
    "> what subject was mentioned, and then what the sentiment was for a given tweet. We've removed the non-relevant messages from\n",
    "> the uploaded dataset.\n",
    "\n",
    "**Details about model**<br>\n",
    " - model contains 3 layers (Embedding, LSTM, Dense with softmax).\n",
    " - Up-sampling is used to balance the data of minority class.\n",
    " - Loss function with different class weight in keras to further reduce class imbalance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "24f5d09448a55ff218617b74ce635b85ca7af80b"
   },
   "source": [
    "\n",
    "### Importing useful packages\n",
    "Lets first import all libraries. Please make sure that you have these libraries installed.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "6c53202d-5c34-4859-e7e9-8ef5c7068287",
    "_uuid": "717bb968c36b9325c7d4cae5724a3672e49ff243"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "c:\\users\\asif\\anaconda3\\envs\\tensor_experiment\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\asif\\anaconda3\\envs\\tensor_experiment\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\asif\\anaconda3\\envs\\tensor_experiment\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\asif\\anaconda3\\envs\\tensor_experiment\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\asif\\anaconda3\\envs\\tensor_experiment\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\asif\\anaconda3\\envs\\tensor_experiment\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "c:\\users\\asif\\anaconda3\\envs\\tensor_experiment\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\asif\\anaconda3\\envs\\tensor_experiment\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\asif\\anaconda3\\envs\\tensor_experiment\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\asif\\anaconda3\\envs\\tensor_experiment\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\asif\\anaconda3\\envs\\tensor_experiment\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\asif\\anaconda3\\envs\\tensor_experiment\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.utils import resample\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2bc2702e-d6f4-df5f-b80e-50ab23a6d29e",
    "_uuid": "9b520acffb5cd85d0e1ada968ad0f12cee33a4b5"
   },
   "source": [
    "### Data Preprocessing\n",
    "- reading the data\n",
    "- kepping only neccessary columns\n",
    "- droping \"Neutral\" sentiment data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "89c8c923-c0bf-7b35-9ab8-e63f00b74e5a",
    "_uuid": "d2bc3bbd2ea3961c49e6673145a0a7226c160e58"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"E:/Python Imarticus/Exam-Deep Learning & UnSuperVised Learning/Sentiment.csv\")\n",
    "# Keeping only the neccessary columns\n",
    "data = data[['text','sentiment']]\n",
    "data = data[data.sentiment != \"Neutral\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "68989f3bc936825f4425d2d08467ce17c4a2f092"
   },
   "source": [
    "Let See the few lines of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "f6a102c71c8e281450f7e73a5678cc9d0bb99e99"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @ScottWalker: Didn't catch the full #GOPdeb...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @RobGeorge: That Carly Fiorina is trending ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @DanScavino: #GOPDebate w/ @realDonaldTrump...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RT @GregAbbott_TX: @TedCruz: \"On my first day ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RT @warriorwoman91: I liked her and was happy ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment\n",
       "1  RT @ScottWalker: Didn't catch the full #GOPdeb...  Positive\n",
       "3  RT @RobGeorge: That Carly Fiorina is trending ...  Positive\n",
       "4  RT @DanScavino: #GOPDebate w/ @realDonaldTrump...  Positive\n",
       "5  RT @GregAbbott_TX: @TedCruz: \"On my first day ...  Positive\n",
       "6  RT @warriorwoman91: I liked her and was happy ...  Negative"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4c0ec63b-cdf8-8e29-812b-0fbbfcea2929",
    "_uuid": "ff12d183224670f9c4c96fd24581b9924d4dff20"
   },
   "source": [
    "> A few things to notice here\n",
    "- \"RT @...\" in start of every tweet\n",
    "- a lot of special characters <br>\n",
    "> We have to remove all this noise also lets convert text into lower case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "43632d2d-6160-12ce-48b0-e5eb1c207076",
    "_uuid": "d0f8b4542106a279f7398db7285ae5e370b2e813"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>scottwalker didnt catch the full gopdebate la...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>robgeorge that carly fiorina is trending  hou...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>danscavino gopdebate w realdonaldtrump delive...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gregabbott_tx tedcruz on my first day i will ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>warriorwoman91 i liked her and was happy when...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment\n",
       "1   scottwalker didnt catch the full gopdebate la...  Positive\n",
       "3   robgeorge that carly fiorina is trending  hou...  Positive\n",
       "4   danscavino gopdebate w realdonaldtrump delive...  Positive\n",
       "5   gregabbott_tx tedcruz on my first day i will ...  Positive\n",
       "6   warriorwoman91 i liked her and was happy when...  Negative"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'] = data['text'].apply(lambda x: x.lower())\n",
    "# removing special chars\n",
    "data['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
    "data['text'] = data['text'].str.replace('rt','')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "43632d2d-6160-12ce-48b0-e5eb1c207076",
    "_uuid": "d0f8b4542106a279f7398db7285ae5e370b2e813"
   },
   "source": [
    "This looks better.<br>\n",
    "Lets pre-process the data so that we can use it to train the model\n",
    "- Tokenize\n",
    "- Padding (to make all sequence of same lengths)\n",
    "- Converting sentiments into numerical data(One-hot form)\n",
    "- train test split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "43632d2d-6160-12ce-48b0-e5eb1c207076",
    "_uuid": "d0f8b4542106a279f7398db7285ae5e370b2e813"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8583, 28) (8583, 2)\n",
      "(2146, 28) (2146, 2)\n"
     ]
    }
   ],
   "source": [
    "max_fatures = 2000\n",
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
    "tokenizer.fit_on_texts(data['text'].values)\n",
    "X = tokenizer.texts_to_sequences(data['text'].values)\n",
    "X = pad_sequences(X)\n",
    "\n",
    "Y = pd.get_dummies(data['sentiment']).values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9753421e-1303-77d5-b17f-5f25fa08c452",
    "_uuid": "aa7d103e946e631133d86ef3adc73e1a8b1a1e89"
   },
   "source": [
    "### Defining model\n",
    "Next, I compose the LSTM Network. Note that **embed_dim**, **lstm_out**, **batch_size**, **droupout_x** variables are hyper parameters, their values are somehow intuitive, can be and must be played with in order to achieve good results. Please also note that I am using softmax as activation function. The reason is that our Network is using categorical crossentropy, and softmax is just the right activation method for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "1ba3cf60-a83c-9c21-05e0-b14303027e93",
    "_uuid": "05cb9ef0ec9e0a4067e3ab7c1bda7b2c1211feda",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 28, 128)           256000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 28, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 196)               254800    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 394       \n",
      "=================================================================\n",
      "Total params: 511,194\n",
      "Trainable params: 511,194\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "15f4ee61-47e4-88c4-4b81-98a85237333f",
    "_uuid": "2dae0f3b95a4ba533453c512e573560a8358e162"
   },
   "source": [
    "### Building LSTM Model\n",
    "Here we train the Network. We should run much more than 15 epoch, but I would have to wait forever (run more epochs later), so it is 15 for now. you will see progress bar (if you want to shut it up use verbose = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "d5e499ac-2eba-6ff7-8d9a-ff65eb04099b",
    "_uuid": "d0b239912cf67294a9f5af6883bb159c44318fc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\asif\\anaconda3\\envs\\tensor_experiment\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/15\n",
      "8583/8583 [==============================] - 17s 2ms/step - loss: 0.4749 - accuracy: 0.7984\n",
      "Epoch 2/15\n",
      "8583/8583 [==============================] - 15s 2ms/step - loss: 0.3361 - accuracy: 0.8554\n",
      "Epoch 3/15\n",
      "8583/8583 [==============================] - 15s 2ms/step - loss: 0.2890 - accuracy: 0.8814\n",
      "Epoch 4/15\n",
      "8583/8583 [==============================] - 16s 2ms/step - loss: 0.2665 - accuracy: 0.8906\n",
      "Epoch 5/15\n",
      "8583/8583 [==============================] - 16s 2ms/step - loss: 0.2518 - accuracy: 0.8981\n",
      "Epoch 6/15\n",
      "8583/8583 [==============================] - 16s 2ms/step - loss: 0.2315 - accuracy: 0.9046\n",
      "Epoch 7/15\n",
      "8583/8583 [==============================] - 15s 2ms/step - loss: 0.2133 - accuracy: 0.9134\n",
      "Epoch 8/15\n",
      "8583/8583 [==============================] - 15s 2ms/step - loss: 0.2111 - accuracy: 0.9089\n",
      "Epoch 9/15\n",
      "8583/8583 [==============================] - 18s 2ms/step - loss: 0.1905 - accuracy: 0.9235\n",
      "Epoch 10/15\n",
      "8583/8583 [==============================] - 20s 2ms/step - loss: 0.1763 - accuracy: 0.9267\n",
      "Epoch 11/15\n",
      "8583/8583 [==============================] - 18s 2ms/step - loss: 0.1719 - accuracy: 0.9300\n",
      "Epoch 12/15\n",
      "8583/8583 [==============================] - 16s 2ms/step - loss: 0.1589 - accuracy: 0.9357\n",
      "Epoch 13/15\n",
      "8583/8583 [==============================] - 16s 2ms/step - loss: 0.1651 - accuracy: 0.9317\n",
      "Epoch 14/15\n",
      "8583/8583 [==============================] - 16s 2ms/step - loss: 0.1488 - accuracy: 0.9396\n",
      "Epoch 15/15\n",
      "8583/8583 [==============================] - 15s 2ms/step - loss: 0.1514 - accuracy: 0.9356\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x27e4535de48>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "model.fit(X_train, Y_train, epochs = 15, batch_size=batch_size, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4ebd7bc1-53c0-0e31-a0b0-b6d0a3017434",
    "_uuid": "47e99d7ed1f27a85eb01dbafc71b66b329fb1d12"
   },
   "source": [
    "### Let evaluate the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "4701961e44bd243e505fc2c1b53b323311ad2b80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix [[1365  334]\n",
      " [ 125  322]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.80      0.86      1699\n",
      "           1       0.49      0.72      0.58       447\n",
      "\n",
      "    accuracy                           0.79      2146\n",
      "   macro avg       0.70      0.76      0.72      2146\n",
      "weighted avg       0.83      0.79      0.80      2146\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict_classes(X_test,batch_size = batch_size)\n",
    "df_test = pd.DataFrame({'true': Y_test.tolist(), 'pred':Y_pred})\n",
    "df_test['true'] = df_test['true'].apply(lambda x: np.argmax(x))\n",
    "print(\"confusion matrix\",confusion_matrix(df_test.true, df_test.pred))\n",
    "print(classification_report(df_test.true, df_test.pred))s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "018ebf39-9414-27d0-232c-a34de051feaf",
    "_uuid": "4b54f18bbf22a953c60f271c318cb076e684df9c"
   },
   "source": [
    "\n",
    "## Solving data imbalance problem\n",
    "\n",
    "**Up-sample Minority Class**\n",
    "\n",
    "Up-sampling is the process of randomly duplicating observations from the minority class in order to reinforce its signal. There are several heuristics for doing so, but the most common way is to simply re-sample with replacement.\n",
    "\n",
    "It's important that we separate test set before up-sampling because after up-sampling there will be multiple copies of same data point and if we do train test split after up-sampling the test set will not be completely unseen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "384ed509b57715fbc7cce5ad37802a8785603b52"
   },
   "outputs": [],
   "source": [
    "# Separate majority and minority classes\n",
    "data_majority = data[data['sentiment'] == 'Negative']\n",
    "data_minority = data[data['sentiment'] == 'Positive']\n",
    "\n",
    "bias = data_minority.shape[0]/data_majority.shape[0]\n",
    "# lets split train/test data first then \n",
    "train = pd.concat([data_majority.sample(frac=0.8,random_state=200),\n",
    "         data_minority.sample(frac=0.8,random_state=200)])\n",
    "test = pd.concat([data_majority.drop(data_majority.sample(frac=0.8,random_state=200).index),\n",
    "        data_minority.drop(data_minority.sample(frac=0.8,random_state=200).index)])\n",
    "\n",
    "train = shuffle(train)\n",
    "test = shuffle(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "ce10e582bd894ec271f2587ceb618a9cf8ab03c5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive data in training: 1789\n",
      "negative data in training: 6794\n",
      "positive data in test: 447\n",
      "negative data in test: 1699\n"
     ]
    }
   ],
   "source": [
    "print('positive data in training:',(train.sentiment == 'Positive').sum())\n",
    "print('negative data in training:',(train.sentiment == 'Negative').sum())\n",
    "print('positive data in test:',(test.sentiment == 'Positive').sum())\n",
    "print('negative data in test:',(test.sentiment == 'Negative').sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b1006f3ca7009e0838d74a8fd5a3fee80375bbe0"
   },
   "source": [
    "Now Lets do up-sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "384ed509b57715fbc7cce5ad37802a8785603b52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majority class before upsample: (6794, 2)\n",
      "minority class before upsample: (1789, 2)\n",
      "After upsampling\n",
      "Negative    6794\n",
      "Positive    6794\n",
      "Name: sentiment, dtype: int64\n",
      "x_train shape: (13588, 29)\n",
      "x_test shape (2146, 29)\n"
     ]
    }
   ],
   "source": [
    "# Separate majority and minority classes in training data for upsampling \n",
    "data_majority = train[train['sentiment'] == 'Negative']\n",
    "data_minority = train[train['sentiment'] == 'Positive']\n",
    "\n",
    "print(\"majority class before upsample:\",data_majority.shape)\n",
    "print(\"minority class before upsample:\",data_minority.shape)\n",
    "\n",
    "# Upsample minority class\n",
    "data_minority_upsampled = resample(data_minority, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples= data_majority.shape[0],    # to match majority class\n",
    "                                 random_state=123) # reproducible results\n",
    " \n",
    "# Combine majority class with upsampled minority class\n",
    "data_upsampled = pd.concat([data_majority, data_minority_upsampled])\n",
    " \n",
    "# Display new class counts\n",
    "print(\"After upsampling\\n\",data_upsampled.sentiment.value_counts(),sep = \"\")\n",
    "\n",
    "max_fatures = 2000\n",
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
    "tokenizer.fit_on_texts(data['text'].values) # training with whole data\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(data_upsampled['text'].values)\n",
    "X_train = pad_sequences(X_train,maxlen=29)\n",
    "Y_train = pd.get_dummies(data_upsampled['sentiment']).values\n",
    "print('x_train shape:',X_train.shape)\n",
    "\n",
    "X_test = tokenizer.texts_to_sequences(test['text'].values)\n",
    "X_test = pad_sequences(X_test,maxlen=29)\n",
    "Y_test = pd.get_dummies(test['sentiment']).values\n",
    "print(\"x_test shape\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "1ba3cf60-a83c-9c21-05e0-b14303027e93",
    "_uuid": "05cb9ef0ec9e0a4067e3ab7c1bda7b2c1211feda",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 29, 128)           256000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_2 (Spatial (None, 29, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 192)               246528    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 386       \n",
      "=================================================================\n",
      "Total params: 502,914\n",
      "Trainable params: 502,914\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "embed_dim = 128\n",
    "lstm_out = 192\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_fatures, embed_dim,input_length = X_train.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(lstm_out, dropout=0.4, recurrent_dropout=0.4))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2a775979-a930-e627-2963-18557d7bf6e6",
    "_uuid": "8799a667a2c0254cb367c193d86e07ee36d91dd7"
   },
   "source": [
    "Lets define class weights as a dictionary, I have defined weight of majority class to be 1 and of minority class to be a multiple of 1/bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "d5e499ac-2eba-6ff7-8d9a-ff65eb04099b",
    "_uuid": "d0b239912cf67294a9f5af6883bb159c44318fc7",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "13588/13588 [==============================] - 27s 2ms/step - loss: 1.2557 - accuracy: 0.5741\n",
      "Epoch 2/15\n",
      "13588/13588 [==============================] - 26s 2ms/step - loss: 0.8070 - accuracy: 0.7501\n",
      "Epoch 3/15\n",
      "13588/13588 [==============================] - 27s 2ms/step - loss: 0.6680 - accuracy: 0.8140\n",
      "Epoch 4/15\n",
      "13588/13588 [==============================] - 28s 2ms/step - loss: 0.5893 - accuracy: 0.8372\n",
      "Epoch 5/15\n",
      "13588/13588 [==============================] - 23s 2ms/step - loss: 0.5367 - accuracy: 0.8518\n",
      "Epoch 6/15\n",
      "13588/13588 [==============================] - 31s 2ms/step - loss: 0.4889 - accuracy: 0.8669\n",
      "Epoch 7/15\n",
      "13588/13588 [==============================] - 31s 2ms/step - loss: 0.4603 - accuracy: 0.8777\n",
      "Epoch 8/15\n",
      "13588/13588 [==============================] - 40s 3ms/step - loss: 0.4429 - accuracy: 0.8795\n",
      "Epoch 9/15\n",
      "13588/13588 [==============================] - 29s 2ms/step - loss: 0.4085 - accuracy: 0.8920\n",
      "Epoch 10/15\n",
      "13588/13588 [==============================] - 26s 2ms/step - loss: 0.3994 - accuracy: 0.8956\n",
      "Epoch 11/15\n",
      "13588/13588 [==============================] - 24s 2ms/step - loss: 0.3820 - accuracy: 0.9015\n",
      "Epoch 12/15\n",
      "13588/13588 [==============================] - 24s 2ms/step - loss: 0.3587 - accuracy: 0.9090\n",
      "Epoch 13/15\n",
      "13588/13588 [==============================] - 24s 2ms/step - loss: 0.3410 - accuracy: 0.9128\n",
      "Epoch 14/15\n",
      "13588/13588 [==============================] - 22s 2ms/step - loss: 0.3332 - accuracy: 0.9154\n",
      "Epoch 15/15\n",
      "13588/13588 [==============================] - 24s 2ms/step - loss: 0.3230 - accuracy: 0.9149\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x27e4d3b1b70>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "# also adding weights\n",
    "class_weights = {0: 1 ,\n",
    "                1: 1.6/bias }\n",
    "model.fit(X_train, Y_train, epochs = 15, batch_size=batch_size, verbose = 1,\n",
    "          class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "2803c5699e0aec22463aadccd1255e63155c1b09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix [[1329  370]\n",
      " [ 115  332]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.78      0.85      1699\n",
      "           1       0.47      0.74      0.58       447\n",
      "\n",
      "    accuracy                           0.77      2146\n",
      "   macro avg       0.70      0.76      0.71      2146\n",
      "weighted avg       0.83      0.77      0.79      2146\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict_classes(X_test,batch_size = batch_size)\n",
    "df_test = pd.DataFrame({'true': Y_test.tolist(), 'pred':Y_pred})\n",
    "df_test['true'] = df_test['true'].apply(lambda x: np.argmax(x))\n",
    "print(\"confusion matrix\",confusion_matrix(df_test.true, df_test.pred))\n",
    "print(classification_report(df_test.true, df_test.pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "085294a5409b58c2188019177041ceb1756f1826"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "13588/13588 [==============================] - 23s 2ms/step - loss: 0.3272 - accuracy: 0.9154\n",
      "Epoch 2/15\n",
      "13588/13588 [==============================] - 22s 2ms/step - loss: 0.3087 - accuracy: 0.9232\n",
      "Epoch 3/15\n",
      "13588/13588 [==============================] - 22s 2ms/step - loss: 0.3029 - accuracy: 0.9236\n",
      "Epoch 4/15\n",
      "13588/13588 [==============================] - 22s 2ms/step - loss: 0.2914 - accuracy: 0.9268\n",
      "Epoch 5/15\n",
      "13588/13588 [==============================] - 22s 2ms/step - loss: 0.2852 - accuracy: 0.9284\n",
      "Epoch 6/15\n",
      "13588/13588 [==============================] - 22s 2ms/step - loss: 0.2849 - accuracy: 0.9279\n",
      "Epoch 7/15\n",
      "13588/13588 [==============================] - 22s 2ms/step - loss: 0.2768 - accuracy: 0.9306\n",
      "Epoch 8/15\n",
      "13588/13588 [==============================] - 22s 2ms/step - loss: 0.2654 - accuracy: 0.9342\n",
      "Epoch 9/15\n",
      "13588/13588 [==============================] - 22s 2ms/step - loss: 0.2566 - accuracy: 0.9356\n",
      "Epoch 10/15\n",
      "13588/13588 [==============================] - 22s 2ms/step - loss: 0.2715 - accuracy: 0.9310\n",
      "Epoch 11/15\n",
      "13588/13588 [==============================] - 23s 2ms/step - loss: 0.2618 - accuracy: 0.9335\n",
      "Epoch 12/15\n",
      "13588/13588 [==============================] - 22s 2ms/step - loss: 0.2453 - accuracy: 0.9378\n",
      "Epoch 13/15\n",
      "13588/13588 [==============================] - 22s 2ms/step - loss: 0.2431 - accuracy: 0.9394\n",
      "Epoch 14/15\n",
      "13588/13588 [==============================] - 22s 2ms/step - loss: 0.2466 - accuracy: 0.9371\n",
      "Epoch 15/15\n",
      "13588/13588 [==============================] - 22s 2ms/step - loss: 0.2466 - accuracy: 0.9389\n",
      "confusion matrix [[1365  334]\n",
      " [ 125  322]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.80      0.86      1699\n",
      "           1       0.49      0.72      0.58       447\n",
      "\n",
      "    accuracy                           0.79      2146\n",
      "   macro avg       0.70      0.76      0.72      2146\n",
      "weighted avg       0.83      0.79      0.80      2146\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# running model to few more epochs\n",
    "model.fit(X_train, Y_train, epochs = 15, batch_size=batch_size, verbose = 1,\n",
    "          class_weight=class_weights)\n",
    "Y_pred = model.predict_classes(X_test,batch_size = batch_size)\n",
    "df_test = pd.DataFrame({'true': Y_test.tolist(), 'pred':Y_pred})\n",
    "df_test['true'] = df_test['true'].apply(lambda x: np.argmax(x))\n",
    "print(\"confusion matrix\",confusion_matrix(df_test.true, df_test.pred))\n",
    "print(classification_report(df_test.true, df_test.pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Based on the model, check the sentiment for the following two sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "24c64f46-edd1-8d0b-7c7c-ef50fd26b2fd",
    "_uuid": "d9aac68e2013b3beffb6a764cc5b85be83073e66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0  33   5   7 144 336]]\n",
      "positive\n"
     ]
    }
   ],
   "source": [
    "twt = ['He is a great leader']\n",
    "#vectorizing the tweet by the pre-fitted tokenizer instance\n",
    "twt = tokenizer.texts_to_sequences(twt)\n",
    "#padding the tweet to have exactly the same shape as `embedding_2` input\n",
    "twt = pad_sequences(twt, maxlen=29, dtype='int32', value=0)\n",
    "print(twt)\n",
    "sentiment = model.predict(twt,batch_size=1,verbose = 2)[0]\n",
    "if(np.argmax(sentiment) == 0):\n",
    "    print(\"negative\")\n",
    "elif (np.argmax(sentiment) == 1):\n",
    "    print(\"positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "53b88b4e6b56cf0e00084e3a41506d9abb5127e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0  33   5   7 988 336]]\n",
      "negative\n"
     ]
    }
   ],
   "source": [
    "twt = ['He is a terrible leader']\n",
    "#vectorizing the tweet by the pre-fitted tokenizer instance\n",
    "twt = tokenizer.texts_to_sequences(twt)\n",
    "#padding the tweet to have exactly the same shape as `embedding_2` input\n",
    "twt = pad_sequences(twt, maxlen=29, dtype='int32', value=0)\n",
    "print(twt)\n",
    "sentiment = model.predict(twt,batch_size=1,verbose = 2)[0]\n",
    "if(np.argmax(sentiment) == 0):\n",
    "    print(\"negative\")\n",
    "elif (np.argmax(sentiment) == 1):\n",
    "    print(\"positive\")"
   ]
  }
 ],
 "metadata": {
  "_change_revision": 0,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
